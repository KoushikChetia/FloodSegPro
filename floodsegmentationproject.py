# -*- coding: utf-8 -*-
"""FloodSegmentationProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C9DVWIzhEs-WHmN93K-tNxwNV_Z98SPI

Importing the necessary libraries
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
from os import listdir

from google.colab import drive
drive.mount('/content/drive')

"""Data Preprocessing"""

def load_data(image_dir, mask_dir, image_size):
    x = []
    y = []

    img_height, img_width = image_size

    # List all image files from both directories
    image_files = sorted(os.listdir(image_dir))
    mask_files = sorted(os.listdir(mask_dir))

    # Ensure the filenames match before processing
    for img_file, mask_file in zip(image_files, mask_files):
        # Construct full file paths
        img_path = os.path.join(image_dir, img_file)
        mask_path = os.path.join(mask_dir, mask_file)

        # Load and resize image
        image = cv2.imread(img_path)
        image = cv2.resize(image, (img_width, img_height), interpolation=cv2.INTER_CUBIC)
        image = image / 256.0  # Normalize

        # Load and resize mask
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Reading mask in grayscale//////
        mask = cv2.resize(mask, (img_width, img_height), interpolation=cv2.INTER_CUBIC)
        mask = mask / 256.0  # Normalize

        mask = np.expand_dims(mask, axis=-1)

        x.append(image)
        y.append(mask)

    # Convert to numpy arrays
    x = np.array(x)
    y = np.array(y)

    return x, y

image_dir = "/content/drive/MyDrive/Datasets/flood_segementation-dataset/Train/images"
mask_dir = "/content/drive/MyDrive/Datasets/flood_segementation-dataset/Train/masks"

# Define image size
image_size = (256, 256)

# Load data
x, y = load_data(image_dir, mask_dir, image_size)

x.shape

y.shape

def plot_img(x):



        plt.figure(figsize=(15,15))

        for i in range(8):
            plt.subplot(1,8,i+1)
            plt.imshow(x[i])
            plt.title(i)
            plt.axis('off')
        plt.show()

# dist_array=[x,y]

plot_img(x)

plot_img(y)

"""Building the Model

"""

from keras.models import Model
from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda,Cropping2D,BatchNormalization,Activation

def unet_model(input_size=(256, 256, 3)):
    inputs = Input(input_size)

    # Contracting Path
    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)
    c1 = Conv2D(64, (3, 3), activation='relu', padding='same')(c1)
    p1 = MaxPooling2D((2, 2))(c1)

    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(p1)
    c2 = Conv2D(128, (3, 3), activation='relu', padding='same')(c2)
    p2 = MaxPooling2D((2, 2))(c2)

    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(p2)
    c3 = Conv2D(256, (3, 3), activation='relu', padding='same')(c3)
    p3 = MaxPooling2D((2, 2))(c3)

    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(p3)
    c4 = Conv2D(512, (3, 3), activation='relu', padding='same')(c4)
    p4 = MaxPooling2D((2, 2))(c4)

    # Bottleneck
    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(p4)
    c5 = Conv2D(1024, (3, 3), activation='relu', padding='same')(c5)

    # Expansive Path
    u6 = UpSampling2D((2, 2))(c5)
    u6 = concatenate([u6, c4], axis=3)
    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(u6)
    c6 = Conv2D(512, (3, 3), activation='relu', padding='same')(c6)

    u7 = UpSampling2D((2, 2))(c6)
    u7 = concatenate([u7, c3], axis=3)
    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(u7)
    c7 = Conv2D(256, (3, 3), activation='relu', padding='same')(c7)


    u8 = UpSampling2D((2, 2))(c7)
    u8 = concatenate([u8, c2], axis=3)
    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(u8)
    c8 = Conv2D(128, (3, 3), activation='relu', padding='same')(c8)

    u9 = UpSampling2D((2, 2))(c8)
    u9 = concatenate([u9, c1], axis=3)
    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(u9)
    c9 = Conv2D(64, (3, 3), activation='relu', padding='same')(c9)

    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)

    model = Model(inputs=[inputs], outputs=[outputs])
    return model

IMG_HEIGHT = x.shape[1]
IMG_WIDTH  = x.shape[2]
IMG_CHANNELS = x.shape[3]

n_classes = 3

x_classes = len(np.unique(x))
x_classes

n_classes

# model=multi_unet_model(n_classes, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)
model=unet_model(input_size=(256, 256, 3))

from tensorflow.keras.optimizers import Adam
model.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss='binary_crossentropy',
    metrics=['accuracy',dice_coef, iou_metric]
)

model.summary()

print("Input shape:", x.shape)  # Should be (num_samples, 256, 256, 3)
print("Output shape:", y.shape)  # Should be (num_samples, 256, 256, 1)

from tensorflow.keras.callbacks import ReduceLROnPlateau

reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)

"""Fitting the data to the model"""

history1 = model.fit(x, y,
                    batch_size = 16,
                    verbose=1,
                    epochs=50,
                     shuffle=False,
                    validation_split=0.2,  # Use a validation split
                    callbacks=[reduce_lr])

image_dir = "/content/drive/MyDrive/Datasets/flood_segementation-dataset/Test/images"
mask_dir = "/content/drive/MyDrive/Datasets/flood_segementation-dataset/Test/masks"

# Define image size
image_size = (256, 256)

# Load data
x_test, y_test = load_data(image_dir, mask_dir, image_size)

x_test.shape

loss, accuracy = model.evaluate(x_test, y_test)
print(f"Test Loss: {loss}")
print(f"Test Accuracy: {accuracy}")

import tensorflow.keras.backend as K
def dice_coef(y_true, y_pred, smooth=1):
    y_pred_binary = K.round(y_pred)  # Convert probabilities to binary (0 or 1)
    intersection = K.sum(y_true * y_pred_binary)
    return (2. * intersection + smooth) / (K.sum(y_true) + K.sum(y_pred_binary) + smooth)

def iou_metric(y_true, y_pred, smooth=1):
    y_pred_binary = K.round(y_pred)
    intersection = K.sum(y_true * y_pred_binary)
    union = K.sum(y_true) + K.sum(y_pred_binary) - intersection
    return (intersection + smooth) / (union + smooth)

# Paths to test data
test_image_dir = "/content/drive/MyDrive/Datasets/flood_segementation-dataset/Test/images"
test_mask_dir = "/content/drive/MyDrive/Datasets/flood_segementation-dataset/Test/masks"

# Load test data
x_test, y_test = load_data(test_image_dir, test_mask_dir,image_size)

# Evaluate the model
loss, accuracy, dice, iou = model.evaluate(x_test, y_test)
print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")
print(f"Test Dice Coefficient: {dice:.4f}")
print(f"Test IoU: {iou:.4f}")

predictions = model.predict(x_test)

sample_image = x_test[2]
sample_mask = y_test[2]
sample_prediction = predictions[2]

sample_prediction_binary = (sample_prediction > 0.5).astype(np.float32)

def get_colored_mask(mask):
    colormap = plt.get_cmap('viridis')
    colored_mask = colormap(mask.squeeze())
    return colored_mask

def plot_sample(image, true_mask, pred_mask):
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 3, 1)
    plt.title('Input Image')
    plt.imshow(image)
    plt.axis('off')

    plt.subplot(1, 3, 2)
    plt.title('True Mask')
    colored_mask = get_colored_mask(true_mask)
    plt.imshow(colored_mask)
    plt.axis('off')

    plt.subplot(1, 3, 3)
    plt.title('Predicted Mask')
    colored_mask = get_colored_mask(pred_mask)
    plt.imshow(colored_mask)
    plt.axis('off')

    plt.show()

# Plot the sample
plot_sample(sample_image, sample_mask, sample_prediction_binary)